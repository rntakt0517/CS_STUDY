# 0과 1로 숫자와 문자 표현하기

## 1. 숫자 표현

### 1.1 기본 개념

- 컴퓨터는 기본적으로 **0과 1만 이해 가능**
- **비트(bit)**: 컴퓨터가 이해하는 가장 작은 정보 단위
  - **1비트**: 2가지 정보 표현 가능 (0 또는 1)
  - **n비트**: 2^n 가지 정보 표현 가능

### 1.2 정보 단위

- **1바이트** = 8비트
- **1KB** = 1,000바이트
- **1MB** = 1,000KB
- **1GB** = 1,000MB
- **1TB** = 1,000GB

**주의**: 과거에는 1KB를 1,024바이트로 사용하기도 했으나, 현재는 1,000바이트를 표준으로 사용

### 1.3 워드(Word)

- **정의**: CPU가 한 번에 처리할 수 있는 정보의 크기
- **종류**:
  - **하프워드**: 워드의 절반 크기
  - **풀워드**: 워드 크기
  - **더블워드**: 워드의 두 배 크기

---

## 2. 숫자 체계

### 2.1 이진법

- **정의**: 0과 1로 숫자 표현
- **특징**: 숫자가 1을 넘어가는 시점에 자리 올림
- **예시**:
  - 십진법 0, 1, 2, 3, 4, 5, 6, 7
  - 이진법 0, 1, 10, 11, 100, 101, 110, 111

### 2.2 음수 표현: 2의 보수법

1. 모든 0과 1을 뒤집기

2. 1을 더하기
- **예시**:
  - 3의 이진수: 0011
  - 뒤집기: 1100
  - 1 더하기: 1101 (이것이 -3의 2의 보수 표현)

### 2.3 16진법

- **사용 문자**: 0-9, A-F
- **특징**: 숫자가 15를 넘어가는 시점에 자리 올림
- **16진수 표기**:
  - 프로그래밍에서: 0x 접두사 사용 (예: 0xA3)
  - 수학적 표기: 아래 첨자 16 사용 (예: A3₁₆)

---

## 3. 문자 표현

### 3.1 주요 개념

- **문자 집합**: 컴퓨터가 이해할 수 있는 문자의 모음
- **인코딩**: 문자를 0과 1로 변환하는 과정
- **디코딩**: 0과 1로 표현된 문자 코드를 문자로 변환하는 과정

### 3.2 ASCII 코드

- **특징**:
  - 영문 알파벳, 숫자, 일부 특수문자, 제어 문자 표현
  - 7비트 사용 (128개 문자 표현 가능)
- **한계**: 영어 외 다른 언어 표현 불가능

### 3.3 한글 인코딩

1. **완성형**:
   - 완성된 글자에 코드 부여
   - 예: '가', '나', '다' 각각에 고유 코드 부여
2. **조합형**:
   - 자음과 모음에 개별 코드 부여
   - 예: 'ㄱ', 'ㅏ' 등에 개별 코드 부여, 조합하여 사용

## 3.3.1 EUC-KR

- **특징**:
  - 완성형 인코딩 방식
  - 2바이트로 한글 표현
- **한계**:
  - 약 2,350개의 한글만 표현 가능
  - 모든 한글 표현 불가능

---

## 4. 유니코드와 UTF

### 4.1 유니코드

- **정의**: 전 세계의 모든 문자를 통합한 단일 문자 집합
- **특징**:
  - 한글, 영어, 특수문자, 이모티콘 등 다양한 문자 포함
  - 각 문자에 고유한 코드 포인트 부여

### 4.2 UTF (Unicode Transformation Format)

- **정의**: 유니코드 인코딩 방식
- **종류**: UTF-8, UTF-16, UTF-32 등

## 4.2.1 UTF-8

- **특징**:
  - 가변 길이 인코딩 (1~4바이트)
  - 코드 포인트 범위에 따라 바이트 수 결정
- **장점**:
  - ASCII와 호환성 유지
  - 웹에서 가장 많이 사용되는 인코딩 방식

---

## 5. 주의사항

- 웹 서비스 개발 시 글자가 깨지면 다음 사항 확인 필요:
  1. 인코딩 호환성
  2. 사용된 문자가 해당 문자 집합에 포함되어 있는지 여부
